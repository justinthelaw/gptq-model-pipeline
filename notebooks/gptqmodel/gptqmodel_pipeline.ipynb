{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myMhZbYV9zEu"
      },
      "source": [
        "## GPTQModel Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZ-Vz0K29zEx"
      },
      "source": [
        "### Install GPTQModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xmjJmv-RnY_k"
      },
      "outputs": [],
      "source": [
        "# clone GPTQModel repo\n",
        "!git clone --depth 1 --branch v0.9.9 https://github.com/ModelCloud/GPTQModel.git\n",
        "\n",
        "# compile and install GPTQModel\n",
        "!cd GPTQModel && pip install --no-build-isolation ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-fnTMx19zEx"
      },
      "source": [
        "### Simple GPTQ Quantization\n",
        "\n",
        "Using the VMWare OPen Instruct dataset and NousResearch's Hermes 2 Pro Mistral 7B fine-tune."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4sJBhYRZm3YT"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import logging\n",
        "from gptqmodel import GPTQModel, QuantizeConfig\n",
        "from transformers import AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "\n",
        "\n",
        "pretrained_model_id = \"NousResearch/Hermes-2-Pro-Mistral-7B\"\n",
        "quantized_model_id = \"NousResearch/Hermes-2-Pro-Mistral-7B-4bit-32g\"\n",
        "\n",
        "\n",
        "def get_open_instruct(tokenizer, nsamples, seqlen):\n",
        "    traindata = load_dataset(\"VMware/open-instruct\", \"default\", split=\"train\").filter(\n",
        "        lambda x: len(x[\"response\"]) >= seqlen\n",
        "    )\n",
        "\n",
        "    return [tokenizer(example[\"response\"]) for example in traindata.select(range(nsamples))]\n",
        "\n",
        "\n",
        "def main():\n",
        "    tokenizer = AutoTokenizer.from_pretrained(pretrained_model_id, use_fast=True)\n",
        "\n",
        "    print(\"Loading Open Instruct training data...\")\n",
        "    train_dataset = get_open_instruct(tokenizer, nsamples=512, seqlen=1024)\n",
        "    print(\"Completed loading of Open Instruct training data!\")\n",
        "\n",
        "    quantize_config = QuantizeConfig(\n",
        "        # quantize model to 4-bit\n",
        "        bits=4,\n",
        "        # 128 offer good balance between inference speed and quantization quality\n",
        "        # 32 will increase vRAM usage but increase inferencing quality\n",
        "        group_size=32,\n",
        "        # increase damp if NaN is encountered during `.quantize()` and/or increase calibration dataset size\n",
        "        damp_percent=0.005,\n",
        "        desc_act=True,\n",
        "        static_groups=False,\n",
        "        sym=True,\n",
        "        true_sequential=True,\n",
        "        lm_head=False,\n",
        "        # marlin is vLLM's preferred GPTQ quantization method, which is included in \"gptq\"\n",
        "        quant_method=\"gptq\",\n",
        "    )\n",
        "\n",
        "    # load un-quantized model, the model will always be force loaded into cpu\n",
        "    model = GPTQModel.from_pretrained(pretrained_model_id, quantize_config)\n",
        "\n",
        "    print(\"Beginning quantization...\")\n",
        "    # quantize model, the calibration_dataset should be list of dict whose keys can only be \"input_ids\" and \"attention_mask\"\n",
        "    # with value under torch.LongTensor type.\n",
        "    model.quantize(train_dataset)\n",
        "    print(\"Quantization complete!\")\n",
        "\n",
        "    print(\"Saving quantized model...\")\n",
        "    # save quantized model\n",
        "    model.save_quantized(quantized_model_id)\n",
        "    # save quantized model using safetensors\n",
        "    model.save_quantized(quantized_model_id, use_safetensors=True, max_shard_size=\"4Gb\")\n",
        "    print(\"Saving quantized model complete!\")\n",
        "\n",
        "    # load quantized model, currently only support cpu or single gpu\n",
        "    model = GPTQModel.from_quantized(quantized_model_id, device=\"cuda:0\")\n",
        "\n",
        "    # inference with model.generate\n",
        "    print(\n",
        "        tokenizer.decode(\n",
        "            model.generate(\n",
        "                **tokenizer(\"What is the capital of Jamaica?\", return_tensors=\"pt\").to(\n",
        "                    \"cuda:0\"\n",
        "                )\n",
        "            )[0]\n",
        "        )\n",
        "    )\n",
        "\n",
        "# set logging configuration for GPTQModel\n",
        "logging.basicConfig(\n",
        "    format=\"%(asctime)s %(levelname)s [%(name)s] %(message)s\",\n",
        "    level=logging.INFO,\n",
        "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
        ")\n",
        "\n",
        "# execute main method\n",
        "main()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Upload Quantized Model to Hugging Face"
      ],
      "metadata": {
        "id": "_e0mHODqWRUJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Login to Hugging Face via PAT and then overwrite the existing Git repository with the newly quantized model's files."
      ],
      "metadata": {
        "id": "pJJaNslHWhNz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "NomLtoEzz857"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt -y install git-lfs\n",
        "!git lfs install\n",
        "!git config --global init.defaultBranch main\n",
        "!git config --global user.email \"justin.law@defenseunicorns.com\"\n",
        "!git config --global user.name \"justinthelaw\"\n",
        "!cd /content/NousResearch/Hermes-2-Pro-Mistral-7B-4bit-32g && rm -rf .git\n",
        "!cd /content/NousResearch/Hermes-2-Pro-Mistral-7B-4bit-32g && git init\n",
        "!cd /content/NousResearch/Hermes-2-Pro-Mistral-7B-4bit-32g && git add .\n",
        "!cd /content/NousResearch/Hermes-2-Pro-Mistral-7B-4bit-32g && git commit -m \"initial commit\"\n",
        "!cd /content/NousResearch/Hermes-2-Pro-Mistral-7B-4bit-32g && git remote add origin https://huggingface.co/justinthelaw/Hermes-2-Pro-Mistral-7B-4bit-32g\n",
        "!cd /content/NousResearch/Hermes-2-Pro-Mistral-7B-4bit-32g && git lfs install\n",
        "!cd /content/NousResearch/Hermes-2-Pro-Mistral-7B-4bit-32g && git push -u -f origin main\n"
      ],
      "metadata": {
        "id": "rmtS_G3wzVPJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}