{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Installing Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6RLySGwfm0Ff",
        "outputId": "318f0459-02b3-42d0-acc8-5652654e7230"
      },
      "outputs": [],
      "source": [
        "# uninstall existing dependencies\n",
        "%pip uninstall transformers torch datasets -y\n",
        "\n",
        "# install GPTQModel pre-requisites\n",
        "%pip install torch datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Installing GPTQModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xmjJmv-RnY_k",
        "outputId": "e8b13bbf-6951-478e-b2af-3c79dcc10b0a"
      },
      "outputs": [],
      "source": [
        "# clone GPTQModel repo\n",
        "!git clone --depth 1 --branch v0.9.9 https://github.com/ModelCloud/GPTQModel.git\n",
        "\n",
        "# compile and install GPTQModel\n",
        "!cd GPTQModel && pip install -vvv --no-build-isolation ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Simple GPTQ Quantization\n",
        "\n",
        "Using the WikiText2 dataset and microsoft/Phi-3-mini-128k-instruct."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "4sJBhYRZm3YT",
        "outputId": "cf980252-c5e6-430c-d8ff-e5000872bcde"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from gptqmodel import GPTQModel, QuantizeConfig\n",
        "from transformers import AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "\n",
        "pretrained_model_id = \"microsoft/Phi-3-mini-128k-instruct\"\n",
        "quantized_model_id = \"Phi-3-mini-128k-instruct-4bit-128g\"\n",
        "\n",
        "def get_wikitext2(tokenizer, nsamples, seqlen):\n",
        "    traindata = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\").filter(\n",
        "        lambda x: len(x[\"text\"]) >= seqlen)\n",
        "\n",
        "    return [tokenizer(example[\"text\"]) for example in traindata.select(range(nsamples))]\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def calculate_avg_ppl(model, tokenizer):\n",
        "    from gptqmodel.utils import Perplexity\n",
        "\n",
        "    ppl = Perplexity(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        dataset_path=\"wikitext\",\n",
        "        dataset_name=\"wikitext-2-raw-v1\",\n",
        "        split=\"train\",\n",
        "        text_column=\"text\",\n",
        "    )\n",
        "\n",
        "    all = ppl.calculate(n_ctx=512, n_batch=512)\n",
        "\n",
        "    # average ppl\n",
        "    avg = sum(all) / len(all)\n",
        "\n",
        "    return avg\n",
        "\n",
        "def main():\n",
        "    tokenizer = AutoTokenizer.from_pretrained(pretrained_model_id, use_fast=True)\n",
        "\n",
        "    train_dataset = get_wikitext2(tokenizer, nsamples=1024, seqlen=1024)\n",
        "\n",
        "    quantize_config = QuantizeConfig(\n",
        "        bits=4,  # quantize model to 4-bit\n",
        "        group_size=128,  # it is recommended to set the value to 128\n",
        "    )\n",
        "\n",
        "    # load un-quantized model, the model will always be force loaded into cpu\n",
        "    model = GPTQModel.from_pretrained(pretrained_model_id, quantize_config)\n",
        "\n",
        "    # quantize model, the calibration_dataset should be list of dict whose keys can only be \"input_ids\" and \"attention_mask\"\n",
        "    # with value under torch.LongTensor type.\n",
        "    model.quantize(train_dataset)\n",
        "\n",
        "    # save quantized model\n",
        "    model.save_quantized(quantized_model_id)\n",
        "\n",
        "    # save quantized model using safetensors\n",
        "    model.save_quantized(quantized_model_id, use_safetensors=True)\n",
        "\n",
        "    # load quantized model, currently only support cpu or single gpu\n",
        "    model = GPTQModel.from_quantized(quantized_model_id, device=\"cuda:0\")\n",
        "\n",
        "    # inference with model.generate\n",
        "    print(tokenizer.decode(model.generate(**tokenizer(\"What is the capital of Jamaica?\", return_tensors=\"pt\").to(\"cuda:0\"))[0]))\n",
        "\n",
        "    print(f\"Quantized Model {quantized_model_id} avg PPL is {calculate_avg_ppl(model, tokenizer)}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import logging\n",
        "\n",
        "    logging.basicConfig(\n",
        "        format=\"%(asctime)s %(levelname)s [%(name)s] %(message)s\",\n",
        "        level=logging.INFO,\n",
        "        datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
        "    )\n",
        "\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOYAjKWxbBGZxHKCbQjfPR2",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
