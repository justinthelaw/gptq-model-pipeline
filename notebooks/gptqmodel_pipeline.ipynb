{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myMhZbYV9zEu"
      },
      "source": [
        "## GPTQModel Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZ-Vz0K29zEx"
      },
      "source": [
        "### Install GPTQModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xmjJmv-RnY_k",
        "outputId": "8a6e66a1-24a9-4ff6-baa8-8f075e49a908"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'GPTQModel'...\n",
            "remote: Enumerating objects: 210, done.\u001b[K\n",
            "remote: Counting objects: 100% (210/210), done.\u001b[K\n",
            "remote: Compressing objects: 100% (176/176), done.\u001b[K\n",
            "Receiving objects: 100% (210/210), 200.83 KiB | 10.57 MiB/s, done.\n",
            "remote: Total 210 (delta 35), reused 90 (delta 28), pack-reused 0\u001b[K\n",
            "Resolving deltas: 100% (35/35), done.\n",
            "Note: switching to '519fbe3ef02335c58e3aa8e9353f8346a8780b91'.\n",
            "\n",
            "You are in 'detached HEAD' state. You can look around, make experimental\n",
            "changes and commit them, and you can discard any commits you make in this\n",
            "state without impacting any branches by switching back to a branch.\n",
            "\n",
            "If you want to create a new branch to retain commits you create, you may\n",
            "do so (now or later) by using -c with the switch command. Example:\n",
            "\n",
            "  git switch -c <new-branch-name>\n",
            "\n",
            "Or undo this operation with:\n",
            "\n",
            "  git switch -\n",
            "\n",
            "Turn off this advice by setting config variable advice.detachedHead to false\n",
            "\n",
            "Processing /content/GPTQModel\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: accelerate>=0.31.0 in /usr/local/lib/python3.10/dist-packages (from gptqmodel==0.9.9+cu1222) (0.32.1)\n",
            "Collecting datasets>=2.20.0 (from gptqmodel==0.9.9+cu1222)\n",
            "  Using cached datasets-2.20.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting sentencepiece>=0.2.0 (from gptqmodel==0.9.9+cu1222)\n",
            "  Using cached sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
            "Requirement already satisfied: numpy>=1.26.4 in /usr/local/lib/python3.10/dist-packages (from gptqmodel==0.9.9+cu1222) (1.26.4)\n",
            "Collecting rouge>=1.0.1 (from gptqmodel==0.9.9+cu1222)\n",
            "  Using cached rouge-1.0.1-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting gekko>=1.1.1 (from gptqmodel==0.9.9+cu1222)\n",
            "  Using cached gekko-1.2.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from gptqmodel==0.9.9+cu1222) (2.3.1+cu121)\n",
            "Requirement already satisfied: triton>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from gptqmodel==0.9.9+cu1222) (2.3.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from gptqmodel==0.9.9+cu1222) (0.4.3)\n",
            "Collecting transformers>=4.43.1 (from gptqmodel==0.9.9+cu1222)\n",
            "  Using cached transformers-4.43.3-py3-none-any.whl.metadata (43 kB)\n",
            "Requirement already satisfied: tqdm>=4.66.4 in /usr/local/lib/python3.10/dist-packages (from gptqmodel==0.9.9+cu1222) (4.66.4)\n",
            "Requirement already satisfied: threadpoolctl>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from gptqmodel==0.9.9+cu1222) (3.5.0)\n",
            "Requirement already satisfied: packaging>=24.1 in /usr/local/lib/python3.10/dist-packages (from gptqmodel==0.9.9+cu1222) (24.1)\n",
            "Collecting ninja>=1.11.1.1 (from gptqmodel==0.9.9+cu1222)\n",
            "  Using cached ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl.metadata (5.3 kB)\n",
            "Collecting protobuf>=4.25.3 (from gptqmodel==0.9.9+cu1222)\n",
            "  Using cached protobuf-5.27.3-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
            "Collecting intel_extension_for_transformers>=1.4.2 (from gptqmodel==0.9.9+cu1222)\n",
            "  Using cached intel_extension_for_transformers-1.4.2-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (26 kB)\n",
            "Collecting auto-round==0.2 (from gptqmodel==0.9.9+cu1222)\n",
            "  Using cached auto_round-0.2-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting auto-gptq (from auto-round==0.2->gptqmodel==0.9.9+cu1222)\n",
            "  Using cached auto_gptq-0.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from auto-round==0.2->gptqmodel==0.9.9+cu1222) (9.0.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.31.0->gptqmodel==0.9.9+cu1222) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.31.0->gptqmodel==0.9.9+cu1222) (6.0.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.31.0->gptqmodel==0.9.9+cu1222) (0.23.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.20.0->gptqmodel==0.9.9+cu1222) (3.15.4)\n",
            "Collecting pyarrow>=15.0.0 (from datasets>=2.20.0->gptqmodel==0.9.9+cu1222)\n",
            "  Using cached pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.20.0->gptqmodel==0.9.9+cu1222) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets>=2.20.0->gptqmodel==0.9.9+cu1222)\n",
            "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=2.20.0->gptqmodel==0.9.9+cu1222) (2.1.4)\n",
            "Collecting requests>=2.32.2 (from datasets>=2.20.0->gptqmodel==0.9.9+cu1222)\n",
            "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting xxhash (from datasets>=2.20.0->gptqmodel==0.9.9+cu1222)\n",
            "  Using cached xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datasets>=2.20.0->gptqmodel==0.9.9+cu1222)\n",
            "  Using cached multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.5.0,>=2023.1.0 (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets>=2.20.0->gptqmodel==0.9.9+cu1222)\n",
            "  Using cached fsspec-2024.5.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.20.0->gptqmodel==0.9.9+cu1222) (3.9.5)\n",
            "Collecting schema (from intel_extension_for_transformers>=1.4.2->gptqmodel==0.9.9+cu1222)\n",
            "  Using cached schema-0.7.7-py2.py3-none-any.whl.metadata (34 kB)\n",
            "Collecting neural-compressor (from intel_extension_for_transformers>=1.4.2->gptqmodel==0.9.9+cu1222)\n",
            "  Using cached neural_compressor-2.6-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge>=1.0.1->gptqmodel==0.9.9+cu1222) (1.16.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->gptqmodel==0.9.9+cu1222) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->gptqmodel==0.9.9+cu1222) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->gptqmodel==0.9.9+cu1222) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->gptqmodel==0.9.9+cu1222) (3.1.4)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=2.0.0->gptqmodel==0.9.9+cu1222)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=2.0.0->gptqmodel==0.9.9+cu1222)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=2.0.0->gptqmodel==0.9.9+cu1222)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=2.0.0->gptqmodel==0.9.9+cu1222)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=2.0.0->gptqmodel==0.9.9+cu1222)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=2.0.0->gptqmodel==0.9.9+cu1222)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=2.0.0->gptqmodel==0.9.9+cu1222)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=2.0.0->gptqmodel==0.9.9+cu1222)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=2.0.0->gptqmodel==0.9.9+cu1222)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=2.0.0->gptqmodel==0.9.9+cu1222)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=2.0.0->gptqmodel==0.9.9+cu1222)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.0.0->gptqmodel==0.9.9+cu1222)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.43.1->gptqmodel==0.9.9+cu1222) (2024.5.15)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.43.1->gptqmodel==0.9.9+cu1222) (0.19.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.20.0->gptqmodel==0.9.9+cu1222) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.20.0->gptqmodel==0.9.9+cu1222) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.20.0->gptqmodel==0.9.9+cu1222) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.20.0->gptqmodel==0.9.9+cu1222) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.20.0->gptqmodel==0.9.9+cu1222) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.20.0->gptqmodel==0.9.9+cu1222) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.20.0->gptqmodel==0.9.9+cu1222) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.20.0->gptqmodel==0.9.9+cu1222) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.20.0->gptqmodel==0.9.9+cu1222) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.20.0->gptqmodel==0.9.9+cu1222) (2024.7.4)\n",
            "Collecting peft>=0.5.0 (from auto-gptq->auto-round==0.2->gptqmodel==0.9.9+cu1222)\n",
            "  Using cached peft-0.12.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0.0->gptqmodel==0.9.9+cu1222) (2.1.5)\n",
            "Collecting deprecated>=1.2.13 (from neural-compressor->intel_extension_for_transformers>=1.4.2->gptqmodel==0.9.9+cu1222)\n",
            "  Using cached Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (from neural-compressor->intel_extension_for_transformers>=1.4.2->gptqmodel==0.9.9+cu1222) (4.10.0.84)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from neural-compressor->intel_extension_for_transformers>=1.4.2->gptqmodel==0.9.9+cu1222) (9.4.0)\n",
            "Requirement already satisfied: prettytable in /usr/local/lib/python3.10/dist-packages (from neural-compressor->intel_extension_for_transformers>=1.4.2->gptqmodel==0.9.9+cu1222) (3.10.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from neural-compressor->intel_extension_for_transformers>=1.4.2->gptqmodel==0.9.9+cu1222) (1.3.2)\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.10/dist-packages (from neural-compressor->intel_extension_for_transformers>=1.4.2->gptqmodel==0.9.9+cu1222) (2.0.8)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.20.0->gptqmodel==0.9.9+cu1222) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.20.0->gptqmodel==0.9.9+cu1222) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.20.0->gptqmodel==0.9.9+cu1222) (2024.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.0.0->gptqmodel==0.9.9+cu1222) (1.3.0)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.13->neural-compressor->intel_extension_for_transformers>=1.4.2->gptqmodel==0.9.9+cu1222) (1.16.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prettytable->neural-compressor->intel_extension_for_transformers>=1.4.2->gptqmodel==0.9.9+cu1222) (0.2.13)\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pycocotools->neural-compressor->intel_extension_for_transformers>=1.4.2->gptqmodel==0.9.9+cu1222) (3.7.1)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->neural-compressor->intel_extension_for_transformers>=1.4.2->gptqmodel==0.9.9+cu1222) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->neural-compressor->intel_extension_for_transformers>=1.4.2->gptqmodel==0.9.9+cu1222) (1.4.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools->neural-compressor->intel_extension_for_transformers>=1.4.2->gptqmodel==0.9.9+cu1222) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools->neural-compressor->intel_extension_for_transformers>=1.4.2->gptqmodel==0.9.9+cu1222) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools->neural-compressor->intel_extension_for_transformers>=1.4.2->gptqmodel==0.9.9+cu1222) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools->neural-compressor->intel_extension_for_transformers>=1.4.2->gptqmodel==0.9.9+cu1222) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools->neural-compressor->intel_extension_for_transformers>=1.4.2->gptqmodel==0.9.9+cu1222) (3.1.2)\n",
            "Using cached auto_round-0.2-py3-none-any.whl (66 kB)\n",
            "Using cached datasets-2.20.0-py3-none-any.whl (547 kB)\n",
            "Using cached gekko-1.2.1-py3-none-any.whl (13.2 MB)\n",
            "Using cached intel_extension_for_transformers-1.4.2-cp310-cp310-manylinux_2_28_x86_64.whl (45.3 MB)\n",
            "Using cached ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
            "Using cached protobuf-5.27.3-cp38-abi3-manylinux2014_x86_64.whl (309 kB)\n",
            "Using cached rouge-1.0.1-py3-none-any.whl (13 kB)\n",
            "Using cached sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Using cached transformers-4.43.3-py3-none-any.whl (9.4 MB)\n",
            "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "Using cached fsspec-2024.5.0-py3-none-any.whl (316 kB)\n",
            "Using cached pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n",
            "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "Using cached auto_gptq-0.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23.5 MB)\n",
            "Using cached multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "Using cached neural_compressor-2.6-py3-none-any.whl (1.5 MB)\n",
            "Using cached schema-0.7.7-py2.py3-none-any.whl (18 kB)\n",
            "Using cached xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "Using cached Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Using cached peft-0.12.0-py3-none-any.whl (296 kB)\n",
            "Using cached nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "Building wheels for collected packages: gptqmodel\n",
            "  Building wheel for gptqmodel (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gptqmodel: filename=gptqmodel-0.9.9+cu1222-cp310-cp310-linux_x86_64.whl size=8892395 sha256=acdc72dae9243304b568db37d53620a2bfe41464eaf24b6c1a6534077d6b92cc\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-46r0gszm/wheels/5f/53/9e/53df737a3ec324dade000b4b31e86fc0ed74b1385198b0dfba\n",
            "Successfully built gptqmodel\n",
            "\u001b[33mWARNING: Error parsing dependencies of fsspec: [Errno 2] No such file or directory: '/usr/local/lib/python3.10/dist-packages/fsspec-2024.6.1.dist-info/METADATA'\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Error parsing dependencies of protobuf: [Errno 2] No such file or directory: '/usr/local/lib/python3.10/dist-packages/protobuf-3.20.3.dist-info/METADATA'\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Error parsing dependencies of pyarrow: [Errno 2] No such file or directory: '/usr/local/lib/python3.10/dist-packages/pyarrow-14.0.2.dist-info/METADATA'\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Error parsing dependencies of requests: [Errno 2] No such file or directory: '/usr/local/lib/python3.10/dist-packages/requests-2.31.0.dist-info/METADATA'\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Error parsing dependencies of sentencepiece: [Errno 2] No such file or directory: '/usr/local/lib/python3.10/dist-packages/sentencepiece-0.1.99.dist-info/METADATA'\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Error parsing dependencies of transformers: [Errno 2] No such file or directory: '/usr/local/lib/python3.10/dist-packages/transformers-4.42.4.dist-info/METADATA'\u001b[0m\u001b[33m\n",
            "\u001b[0mInstalling collected packages: sentencepiece, schema, ninja, xxhash, rouge, requests, pyarrow, protobuf, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, gekko, fsspec, dill, deprecated, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, nvidia-cusolver-cu12, transformers, neural-compressor, datasets, intel_extension_for_transformers, peft, auto-gptq, auto-round, gptqmodel\n",
            "  Attempting uninstall: sentencepiece\n",
            "\u001b[33m    WARNING: No metadata found in /usr/local/lib/python3.10/dist-packages\u001b[0m\u001b[33m\n",
            "\u001b[0m    Found existing installation: sentencepiece 0.1.99\n",
            "    Can't uninstall 'sentencepiece'. No files were found to uninstall.\n",
            "  Attempting uninstall: requests\n",
            "\u001b[33m    WARNING: No metadata found in /usr/local/lib/python3.10/dist-packages\u001b[0m\u001b[33m\n",
            "\u001b[0m    Found existing installation: requests 2.31.0\n",
            "    Can't uninstall 'requests'. No files were found to uninstall.\n",
            "  Attempting uninstall: pyarrow\n",
            "\u001b[33m    WARNING: No metadata found in /usr/local/lib/python3.10/dist-packages\u001b[0m\u001b[33m\n",
            "\u001b[0m    Found existing installation: pyarrow 14.0.2\n",
            "    Can't uninstall 'pyarrow'. No files were found to uninstall.\n",
            "  Attempting uninstall: protobuf\n",
            "\u001b[33m    WARNING: No metadata found in /usr/local/lib/python3.10/dist-packages\u001b[0m\u001b[33m\n",
            "\u001b[0m    Found existing installation: protobuf 3.20.3\n",
            "    Can't uninstall 'protobuf'. No files were found to uninstall.\n",
            "  Attempting uninstall: fsspec\n",
            "\u001b[33m    WARNING: No metadata found in /usr/local/lib/python3.10/dist-packages\u001b[0m\u001b[33m\n",
            "\u001b[0m    Found existing installation: fsspec 2024.6.1\n",
            "    Can't uninstall 'fsspec'. No files were found to uninstall.\n",
            "  Attempting uninstall: transformers\n",
            "\u001b[33m    WARNING: No metadata found in /usr/local/lib/python3.10/dist-packages\u001b[0m\u001b[33m\n",
            "\u001b[0m    Found existing installation: transformers 4.42.4\n",
            "    Can't uninstall 'transformers'. No files were found to uninstall.\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires protobuf<5,>=3.20, but you have protobuf 5.27.3 which is incompatible.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
            "gcsfs 2024.6.1 requires fsspec==2024.6.1, but you have fsspec 2024.5.0 which is incompatible.\n",
            "google-ai-generativelanguage 0.6.6 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.3 which is incompatible.\n",
            "google-cloud-aiplatform 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.3 which is incompatible.\n",
            "google-cloud-bigquery-storage 2.25.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.3 which is incompatible.\n",
            "google-cloud-datastore 2.19.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.3 which is incompatible.\n",
            "google-cloud-firestore 2.16.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.3 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\n",
            "tensorboard 2.17.0 requires protobuf!=4.24.0,<5.0.0,>=3.19.6, but you have protobuf 5.27.3 which is incompatible.\n",
            "tensorflow 2.17.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.27.3 which is incompatible.\n",
            "tensorflow-metadata 1.15.0 requires protobuf<4.21,>=3.20.3; python_version < \"3.11\", but you have protobuf 5.27.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed auto-gptq-0.7.1 auto-round-0.2 datasets-2.20.0 deprecated-1.2.14 dill-0.3.8 fsspec-2024.5.0 gekko-1.2.1 gptqmodel-0.9.9+cu1222 intel_extension_for_transformers-1.4.2 multiprocess-0.70.16 neural-compressor-2.6 ninja-1.11.1.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105 peft-0.12.0 protobuf-5.27.3 pyarrow-17.0.0 requests-2.32.3 rouge-1.0.1 schema-0.7.7 sentencepiece-0.2.0 transformers-4.43.3 xxhash-3.4.1\n"
          ]
        }
      ],
      "source": [
        "# clone GPTQModel repo\n",
        "!git clone --depth 1 --branch v0.9.9 https://github.com/ModelCloud/GPTQModel.git\n",
        "\n",
        "# compile and install GPTQModel\n",
        "!cd GPTQModel && pip install --no-build-isolation ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-fnTMx19zEx"
      },
      "source": [
        "### Simple GPTQ Quantization\n",
        "\n",
        "Using the WikiText2 dataset and microsoft/Phi-3-mini-128k-instruct."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116,
          "referenced_widgets": [
            "452cc5ed09664cdcb7f5a41e3f76822c",
            "332a8ee178044881ae365df87a17a411",
            "fe395d8df7314fe4bea3d29dad14fd79",
            "994014a0db704a85b1421bde47092c41",
            "38f7080b28164a82b9e6dfc760a6fb6e",
            "62d2e38cb5d54b4e999e0cd5d6674642",
            "6ea3ee199c9546cb951fd1e11559945a",
            "50113f0bd9d44a64960adc7785f6dc07",
            "1c81d47309864a22b8dad937a1379772",
            "b52503d1d6db4e7bba9c99e8ed90af79",
            "38d8966b6ea0416b9a52ba96e27c203c",
            "ba2992cb0c9b4a40b55fb26d6a55e4ee",
            "411db0bd9ef84ea1af2e6f8ded8ff8ce",
            "a67a239a40c44fff8d08b38a6aad106c",
            "acfab4caf45f431b8427420c533f4455",
            "daa280c7d6754412a9d30f0da6a7b8d7",
            "4182f2c2234b49f9a1244cfc7baae946",
            "0be408a56cda44fd9d285e537c736770",
            "bad895c447594cdea03f73c2a2e48d45",
            "1b16b8c2db2940d599d929643d48c7b6",
            "110c360c0e6f473fac655c11bde05a19",
            "a45f85eced02464eb2b791837b614032"
          ]
        },
        "id": "4sJBhYRZm3YT",
        "outputId": "a1cc85b7-1974-4d64-c6c0-da6c7bdd3e28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Open Instruct training data...\n",
            "Completed loading of Open Instruct training data!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "452cc5ed09664cdcb7f5a41e3f76822c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model-00001-of-00002.bin:  10%|9         | 954M/9.94G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ba2992cb0c9b4a40b55fb26d6a55e4ee"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "import torch\n",
        "import logging\n",
        "from gptqmodel import GPTQModel, QuantizeConfig\n",
        "from transformers import AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "\n",
        "\n",
        "pretrained_model_id = \"teknium/OpenHermes-2-Mistral-7B\"\n",
        "quantized_model_id = \"teknium/OpenHermes-2-Mistral-7B-4bit-128g\"\n",
        "\n",
        "\n",
        "def get_open_instruct(tokenizer, nsamples, seqlen):\n",
        "    traindata = load_dataset(\"VMware/open-instruct\", \"default\", split=\"train\").filter(\n",
        "        lambda x: len(x[\"response\"]) >= seqlen\n",
        "    )\n",
        "\n",
        "    return [tokenizer(example[\"response\"]) for example in traindata.select(range(nsamples))]\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def calculate_avg_ppl(model, tokenizer):\n",
        "    from gptqmodel.utils import Perplexity\n",
        "\n",
        "    ppl = Perplexity(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        dataset_path=\"VMware/open-instruct\",\n",
        "        dataset_name=\"default\",\n",
        "        split=\"train\",\n",
        "        text_column=\"response\",\n",
        "    )\n",
        "\n",
        "    # n_ctx is context size\n",
        "    # n_batch is the batch size\n",
        "    all = ppl.calculate(n_ctx=128, n_batch=128)\n",
        "\n",
        "    # average ppl\n",
        "    avg = sum(all) / len(all)\n",
        "\n",
        "    return avg\n",
        "\n",
        "\n",
        "def main():\n",
        "    tokenizer = AutoTokenizer.from_pretrained(pretrained_model_id, use_fast=True)\n",
        "\n",
        "    print(\"Loading Open Instruct training data...\")\n",
        "    train_dataset = get_open_instruct(tokenizer, nsamples=512, seqlen=1024)\n",
        "    print(\"Completed loading of Open Instruct training data!\")\n",
        "\n",
        "    quantize_config = QuantizeConfig(\n",
        "        # quantize model to 4-bit\n",
        "        bits=4,\n",
        "        # 128 offer good balance between inference speed and quantization quality\n",
        "        # 32 will increase vRAM usage but increase inferencing quality\n",
        "        group_size=32,\n",
        "        # increase damp if NaN is encountered during `.quantize()` and/or increase calibration dataset size\n",
        "        damp_percent=0.005,\n",
        "        desc_act=True,\n",
        "        static_groups=False,\n",
        "        sym=True,\n",
        "        true_sequential=True,\n",
        "        lm_head=False,\n",
        "        # marlin is vLLM's preferred GPTQ quantization method, which is included in \"gptq\"\n",
        "        quant_method=\"gptq\",\n",
        "    )\n",
        "\n",
        "    # load un-quantized model, the model will always be force loaded into cpu\n",
        "    model = GPTQModel.from_pretrained(pretrained_model_id, quantize_config)\n",
        "\n",
        "    print(\"Beginning quantization...\")\n",
        "    # quantize model, the calibration_dataset should be list of dict whose keys can only be \"input_ids\" and \"attention_mask\"\n",
        "    # with value under torch.LongTensor type.\n",
        "    model.quantize(train_dataset)\n",
        "    print(\"Quantization complete!\")\n",
        "\n",
        "    print(\"Saving quantized model...\")\n",
        "    # save quantized model\n",
        "    model.save_quantized(quantized_model_id)\n",
        "    # save quantized model using safetensors\n",
        "    model.save_quantized(quantized_model_id, use_safetensors=True)\n",
        "    print(\"Saving quantized model complete!\")\n",
        "\n",
        "    # load quantized model, currently only support cpu or single gpu\n",
        "    model = GPTQModel.from_quantized(quantized_model_id, device=\"cuda:0\")\n",
        "\n",
        "    # inference with model.generate\n",
        "    print(\n",
        "        tokenizer.decode(\n",
        "            model.generate(\n",
        "                **tokenizer(\"What is the capital of Jamaica?\", return_tensors=\"pt\").to(\n",
        "                    \"cuda:0\"\n",
        "                )\n",
        "            )[0]\n",
        "        )\n",
        "    )\n",
        "\n",
        "    print(\n",
        "        f\"Quantized Model {quantized_model_id} avg PPL is {calculate_avg_ppl(model, tokenizer)}\"\n",
        "    )\n",
        "\n",
        "# set logging configuration for GPTQModel\n",
        "logging.basicConfig(\n",
        "    format=\"%(asctime)s %(levelname)s [%(name)s] %(message)s\",\n",
        "    level=logging.INFO,\n",
        "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
        ")\n",
        "\n",
        "# execute main method\n",
        "main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "452cc5ed09664cdcb7f5a41e3f76822c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_332a8ee178044881ae365df87a17a411",
              "IPY_MODEL_fe395d8df7314fe4bea3d29dad14fd79",
              "IPY_MODEL_994014a0db704a85b1421bde47092c41"
            ],
            "layout": "IPY_MODEL_38f7080b28164a82b9e6dfc760a6fb6e"
          }
        },
        "332a8ee178044881ae365df87a17a411": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_62d2e38cb5d54b4e999e0cd5d6674642",
            "placeholder": "​",
            "style": "IPY_MODEL_6ea3ee199c9546cb951fd1e11559945a",
            "value": "Downloading shards:   0%"
          }
        },
        "fe395d8df7314fe4bea3d29dad14fd79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_50113f0bd9d44a64960adc7785f6dc07",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1c81d47309864a22b8dad937a1379772",
            "value": 0
          }
        },
        "994014a0db704a85b1421bde47092c41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b52503d1d6db4e7bba9c99e8ed90af79",
            "placeholder": "​",
            "style": "IPY_MODEL_38d8966b6ea0416b9a52ba96e27c203c",
            "value": " 0/2 [00:00&lt;?, ?it/s]"
          }
        },
        "38f7080b28164a82b9e6dfc760a6fb6e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "62d2e38cb5d54b4e999e0cd5d6674642": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ea3ee199c9546cb951fd1e11559945a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "50113f0bd9d44a64960adc7785f6dc07": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c81d47309864a22b8dad937a1379772": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b52503d1d6db4e7bba9c99e8ed90af79": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "38d8966b6ea0416b9a52ba96e27c203c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ba2992cb0c9b4a40b55fb26d6a55e4ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_411db0bd9ef84ea1af2e6f8ded8ff8ce",
              "IPY_MODEL_a67a239a40c44fff8d08b38a6aad106c",
              "IPY_MODEL_acfab4caf45f431b8427420c533f4455"
            ],
            "layout": "IPY_MODEL_daa280c7d6754412a9d30f0da6a7b8d7"
          }
        },
        "411db0bd9ef84ea1af2e6f8ded8ff8ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4182f2c2234b49f9a1244cfc7baae946",
            "placeholder": "​",
            "style": "IPY_MODEL_0be408a56cda44fd9d285e537c736770",
            "value": "pytorch_model-00001-of-00002.bin:  27%"
          }
        },
        "a67a239a40c44fff8d08b38a6aad106c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bad895c447594cdea03f73c2a2e48d45",
            "max": 9943044428,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1b16b8c2db2940d599d929643d48c7b6",
            "value": 2684354560
          }
        },
        "acfab4caf45f431b8427420c533f4455": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_110c360c0e6f473fac655c11bde05a19",
            "placeholder": "​",
            "style": "IPY_MODEL_a45f85eced02464eb2b791837b614032",
            "value": " 2.67G/9.94G [02:30&lt;14:57, 8.10MB/s]"
          }
        },
        "daa280c7d6754412a9d30f0da6a7b8d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4182f2c2234b49f9a1244cfc7baae946": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0be408a56cda44fd9d285e537c736770": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bad895c447594cdea03f73c2a2e48d45": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b16b8c2db2940d599d929643d48c7b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "110c360c0e6f473fac655c11bde05a19": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a45f85eced02464eb2b791837b614032": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}